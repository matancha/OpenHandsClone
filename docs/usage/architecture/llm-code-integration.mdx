---
title: LLM Integration Deep Dive
---

This document explains how the OpenHands codebase orchestrates Large Language Models (LLMs). It focuses on the most relevant modules so presenters can highlight how messages are formatted, prompts are built, state is preserved and how the system interprets structured responses from the model.

## High Level Organization

OpenHands separates responsibilities across several components:

- **`openhands.llm`** – contains the `LLM` class which wraps LiteLLM. It handles API calls, retries, logging, and cost accounting.
- **`openhands.controller`** – manages `Agent` instances and controls the main loop that interacts with the runtime. `AgentController` listens on an `EventStream` and keeps a `State` object with history, metrics and iteration counts.
- **`openhands.events`** – defines `Action` and `Observation` types published on the `EventStream`. Actions generated by the agent are executed by the runtime and produce observations that feed back into the controller.
- **`openhands.agenthub`** – provides concrete agent implementations (e.g. `CodeActAgent`) along with their prompts and tools.

A simplified control flow is documented in `openhands/README.md` and reproduced below:

```python
while True:
  prompt = agent.generate_prompt(state)
  response = llm.completion(prompt)
  action = agent.parse_response(response)
  observation = runtime.run(action)
  state = state.update(action, observation)
```
【F:openhands/README.md†L27-L47】

In practice the pieces communicate via the `EventStream` rather than direct calls. The controller publishes actions, the runtime executes them and posts observations, and the agent reacts to new events.

## Message Formatting

Messages exchanged with the LLM use the `Message` class from `openhands/core/message.py`. Each message contains a role (`user`, `assistant`, `system` or `tool`) and a list of content items. Content can be text or images and the class knows how to serialize itself for LiteLLM:

```python
class Message(BaseModel):
    role: Literal['user', 'system', 'assistant', 'tool']
    content: list[TextContent | ImageContent] = Field(default_factory=list)
    cache_enabled: bool = False
    vision_enabled: bool = False
    function_calling_enabled: bool = False
    tool_calls: list[ChatCompletionMessageToolCall] | None = None
    tool_call_id: str | None = None
    name: str | None = None
```
【F:openhands/core/message.py†L34-L51】

Depending on whether the current model supports prompt caching, vision or function calling, the `serialize_model` method outputs either a simple string or a list of structured items. Tool call metadata is also inserted so the runtime can correlate LLM responses with executed tools.

## State Between Calls

The `State` object (see `openhands/controller/state`) keeps track of iteration counters, cost metrics and the history of events. Every action and observation is appended to this history via the `EventStream`. When the agent completes a step, the controller records updated metrics and persists the state if configured. `run_agent_until_done` in `openhands/core/loop.py` continues looping until the state reaches a terminal status.

## Structured Output and Tool Use

Most agents rely on function calling to obtain structured outputs. For example `CodeActAgent` converts tool calls returned by the LLM into `Action` objects using `function_calling.response_to_actions`:

```python
if assistant_msg.tool_calls:
    for i, tool_call in enumerate(assistant_msg.tool_calls):
        arguments = json.loads(tool_call.function.arguments)
        ...
        action = CmdRunAction(command=arguments['command'], is_input=is_input)
        action.tool_call_metadata = ToolCallMetadata(
            tool_call_id=tool_call.id,
            function_name=tool_call.function.name,
            model_response=response,
            total_calls_in_response=len(assistant_msg.tool_calls),
        )
        actions.append(action)
```
【F:openhands/agenthub/codeact_agent/function_calling.py†L32-L120】【F:openhands/agenthub/codeact_agent/function_calling.py†L120-L180】

This mapping ensures that each tool invocation from the LLM becomes a concrete action. For non‑function‑calling models, the assistant’s text is wrapped in a `MessageAction` so the controller can continue the conversation.

## Prompt Construction

Prompts are built through `PromptManager` (`openhands/utils/prompt.py`). The manager loads Jinja2 templates from an agent’s `prompts/` directory to produce the system message, example user message and additional workspace context. For instance, `CodeActAgent` initializes its manager with:

```python
self._prompt_manager = PromptManager(
    prompt_dir=os.path.join(os.path.dirname(__file__), 'prompts'),
)
```
【F:openhands/agenthub/codeact_agent/codeact_agent.py†L107-L111】

Templates include `system_prompt.j2` which defines the agent’s behaviour and guidelines:

```
You are OpenHands agent, a helpful AI assistant that can interact with a computer to solve tasks.
<ROLE>
...
```
【F:openhands/agenthub/codeact_agent/prompts/system_prompt.j2†L1-L26】

The user prompt template may be empty by default, letting the initial user message supply instructions. Additional templates describe triggered micro‑agents or repository context. PromptManager also has utilities like `add_turns_left_reminder` which appends a reminder to the last user message if the task is nearing the iteration limit.

## Synchronizing Multi‑Prompt Flows

When multiple prompts are required (e.g. a system message followed by in‑context learning examples), `ConversationMemory` and the agent’s `_get_messages` method combine them into a single list before calling the LLM. Message roles are alternated to avoid consecutive user/assistant messages and caching markers are applied when supported. The controller waits for the resulting actions before sending any further prompts, ensuring step‑wise synchronization through the `EventStream`.

## Tips and Tricks

- `LLM` uses the LiteLLM `completion` API. Retries for rate limits and timeouts are handled automatically via `RetryMixin` with exponential backoff.
- For models that lack native function calling, the code converts function calls to textual prompts and back (`convert_fncall_messages_to_non_fncall_messages` and `convert_non_fncall_messages_to_fncall_messages`).
- Token usage and cost metrics are recorded after each call using `_post_completion` so budgets can be enforced.
- Prompt caching is enabled for Anthropic models by setting `cache_enabled` on messages and reading cache hit tokens from the LLM response.

This overview should help presenters focus on the most relevant parts of the code when discussing how OpenHands uses LLMs under the hood.
