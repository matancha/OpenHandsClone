# Caching and Reuse in OpenHands

This document summarizes how OpenHands handles caching of LLM interactions and reuse of prompts or code fragments. It is intended as reference material for seminar discussions.

## Overview

OpenHands keeps a full history of actions and observations for each agent. The conversation context is rebuilt from this history on **every iteration**. When the context window would exceed the model limit, a *condenser* summarizes older events to keep the prompt short. These summaries are inserted back into the history but are not reused across different sessions.

OpenHands does not maintain an internal database of past prompts or completions. Instead it relies on the underlying LLM provider for optional prompt caching support.
### Design Rationale

This approach reflects OpenHands' focus on reproducibility over efficiency. By recomputing completions each loop, the agent ensures outputs always reflect the latest state. Cached responses could introduce inconsistencies if the workspace changes, so they are avoided except for provider-level caching.


## Prompt Caching

The `Message` class includes a `cache_enabled` flag and `cache_prompt` markers. When the LLM is known to support caching (currently Anthropic models), these markers translate to `cache_control` fields in the API payload:

```python
class Message(BaseModel):
    role: Literal['user', 'system', 'assistant', 'tool']
    content: list[TextContent | ImageContent] = Field(default_factory=list)
    cache_enabled: bool = False
```
```python
if self.cache_prompt:
    data['cache_control'] = {'type': 'ephemeral'}
```
{cite}`F:openhands/core/message.py#30-60`

During message formatting, `ConversationMemory.apply_prompt_caching` marks the final user or tool message as cacheable when using Anthropic models:

```python
if len(messages) > 0 and messages[0].role == 'system':
    messages[0].content[-1].cache_prompt = True
for message in reversed(messages):
    if message.role in ('user', 'tool'):
        message.content[-1].cache_prompt = True
        break
```
{cite}`F:openhands/memory/conversation_memory.py#599-612`

The LLM wrapper checks whether prompt caching is enabled for the current model:

```python
def is_caching_prompt_active(self) -> bool:
    return (
        self.config.caching_prompt is True
        and (
            self.config.model in CACHE_PROMPT_SUPPORTED_MODELS
            or self.config.model.split('/')[-1] in CACHE_PROMPT_SUPPORTED_MODELS
        )
    )
```
{cite}`F:openhands/llm/llm.py#535-548`

When responses are returned, `_post_completion` logs any cache hits provided by the LLM provider:

```python
prompt_tokens_details: PromptTokensDetails = usage.get('prompt_tokens_details')
cache_hit_tokens = (
    prompt_tokens_details.cached_tokens
    if prompt_tokens_details and prompt_tokens_details.cached_tokens
    else 0
)
```
{cite}`F:openhands/llm/llm.py#598-608`

The configuration option `caching_prompt` defaults to `true` so caching is attempted when the provider supports it:

```text
- `caching_prompt`
  - Type: `bool`
  - Default: `true`
  - Description: Using the prompt caching feature if provided by the LLM and supported
```
{cite}`F:docs/usage/configuration-options.mdx#251-254`

This mechanism does **not** store cached prompts locally; it simply exposes provider-level caching.

## Code Reuse and Deduplication

OpenHands does not keep a library of previous completions. The seminar notes explicitly mention that no vector store or scratchpad is implemented:

```text
The repository search shows no implementation of a vector store or external knowledge base. Memory is derived solely from the event history and the optional microagent files. Summaries are produced by the LLM itself via the condenser; there is no retrieval from a vector database or long‑term scratchpad beyond the saved `State` pickles.
```
{cite}`F:docs/seminar/memory-and-context.md#169-177`

Because prompts are rebuilt each time, similar subtasks are recomputed rather than deduplicated. Summaries generated by the condenser help reduce context size but are **not** reused across different requests. No hashing of prompts or caching of utility functions occurs.

## Prompt Construction Efficiency

The prompt is rebuilt from the condensed event history on every loop iteration:

```python
current_view = View.from_events(self.state.history)
kept_events = self._apply_conversation_window(current_view.events)
```
{cite}`F:openhands/controller/agent_controller.py#1164-1167`

The structured summary condenser generates a new summary whenever history needs to be shortened:

```python
response = self.llm.completion(
    messages=self.llm.format_messages_for_llm(messages),
    tools=[StateSummary.tool_description()],
    tool_choice={'type': 'function', 'function': {'name': 'create_state_summary'}},
)
```
{cite}`F:openhands/memory/condenser/impl/structured_summary_condenser.py#218-258`

These summaries replace forgotten events but are not cached for reuse elsewhere. Tool responses and LLM plans are also recomputed each time; there is no memoization layer.

## Other Cached Elements

Some non-LLM aspects are cached for performance, such as:

- **Event history pages** in `EventStore` to avoid excessive file reads.
- **Diff visualizations** in `FileEditObservation` so repeated `__str__` calls reuse the computed diff.

These caches improve performance of the runtime but do not eliminate LLM calls.

## Summary: Cached vs Recomputed

| Item | Cached? | Notes |
| --- | --- | --- |
| LLM prompt completions | ❌ (except provider-level) | Each iteration recomputes the entire prompt |
| Prompt structure (microagents, workspace info) | ❌ | Rebuilt from state and templates |
| Condensed summaries | ✅ within session | Stored in event history but not reused across sessions |
| Code completions or utility functions | ❌ | No hashing or similarity search |
| Diff visualizations and event history pages | ✅ | Cached locally for performance |


## Conclusion

OpenHands prioritizes reproducibility and explicit history over aggressive reuse. LLM prompts are regenerated from the condensed event log every iteration. The only LLM caching available is the provider-level prompt caching for supported models, controlled via the `caching_prompt` setting. There is no semantic deduplication of generated code or reuse of previous completions. Summaries help manage context but do not serve as a cache of previous work.

## Future Work

In future versions, OpenHands could explore lightweight caching or fingerprinting of generated code segments. Storing hashed prompt-response pairs or common utilities might reduce recomputation without compromising the audit trail.


